---
title: "IODS Final Assignment"
author: 
- name: "Maija Absetz"  
  affiliation: "maija.absetz@helsinki.fi"
date: "8.3.2017"
output: 
  html_document:
    theme: readable
    highlight: tango
    toc: true
    toc_depth: 2
    toc_floath: true
    fig_caption: true
    fig_width: 6
    fig_height: 5
    code_folding: hide
  
---
#Abstract

This is the final assignment for open data course, where data from human development programme will be used. Focus of this assignment will be on expected years of education and I will perform LDA analysis to find out what are the variables that determine the level of education.In addition, LDA results with target variable expected years of education will be compared to LDA with clusters as target variables. Both find mother's mortality rate being in significant role and life expectancy and mean yeras of education in additional place to determine the expected years of education.

#1 Introduction to open data science
##1.1 What have we done so far

This is a sample of a progress of a Bachelor level historian developing her skills with statistics with R-software. I have statistics as a minor and just started with R programming language. My message to all of you is: If I can do this, so can you. I'm not saying it's easy, but I'm saying it's possible.

This course has been an introductory course to R-software using R-studio, Github and DataCamp. Through these weeks we have learned how to use R.markdown files and how to push changes from R.mrakdown files to an html web page with Github. We have also learned different data wrangling and analysing techniques briefly with data camp: [link](https://www.datacamp.com/)

What I've done so far in this course can be seen at my course diary: [link](https://macabset.github.io/IODS-project/). We have had weekly assignments regarding a different data analysing method each week.

##1.2 What lies ahead of us
This is the final assignment for the course introduction to open data science. In this assignment we create our own web pages from the scratch (Oh, yes, you're reading it) and perform statistical analysis with data chosen.

At my Github repository anyone can see the R.markdown codes for this web page. Basically, it means that if you have Rstudio and Github username, YOU can make your own web page, too. How cool is that! Link to my Github repository is as promised: [link](https://github.com/macabset/IODS-final)

If you keep up with me, you will see how real life data is wrangled so that classes and groups from the data can be predicted and visualized. I'll do my best to explain every step adequately.

#2 Data: human

I'm going to use my human data from previous exercises. The dataset originates from United Nations Development Programme, and you can read more about it from here: [link](http://hdr.undp.org/en/content/human-development-index-hdi). 

I find this data the most interesting one we've used, since it's actual data that describes the wellbeing of nations in a very multilinear way. Instead of just using Gross National Product or Index as an explaining variable, we can take inequality or education levels as explanatory variables when explaining the wellbeing of a state. This way we do not compare nations simply based on their wealth but based on their human capacity. 

##2.1 Variables

I have created a subset called "human2", since we have already used human data previously. The data consists of 10 variables and 155 observations. Variables modified from original dataset are Country, Edu2FM, LabFM and GNI. All variables are numeric and continuous. For further information about modifications made, you can go and check my Github repository: [link](https://github.com/macabset/IODS-final/blob/master/data/human_data2.R)


```{r}
#Bringing data to R.markdown + summaries
getwd()
human2 <- read.csv("data/human2.csv", sep=",", header = TRUE, row.names = 1)

str(human2)
dim(human2)

head(human2)
```
The data wrangling done here is strongly based on the original ch4 workout. In addition I have added variable HDI and mean years of education. I hope these additions will provide more of background knowledge to the evaluations of the connections. Later on I will continue data wrangling for the analysing part.

Underneath are the names of the 10 variables and their description.

  + Country: used as row names, regions not included

  + Edu2FM: ratio of females to males in over secondary level education
  + LabFM: ratio of female labour force to male labour force
  + Edu.exp: Expected years of education
  + Life.exp: Life expectancy at birth
  + GNI: Gross National Income per capita
  + Mot.mor: Mothers' mortality ratio
  + Adol.birth: Adolescent birth rate
  + Parl.F: Percentage of female representatives in parliament
  + HDI: human development index. Combination of education, life expectation and GNI.
  + Edu.y.mean: mean years of education/ Country

In addition I have excluded missing values from our data set by removing them.

```{r}
#Complete cases, whithout missing values
complete.cases(human2)

```
```{r}
#Packages we may need in this workout
library(GGally)
library(dplyr)
library(corrplot)
library(FactoMineR)
library(ggplot2)
library(tidyr)
library(MASS)
```

##2.2 Picturing human

Let's do some correlation matrices and plots to visualize them in order to notice the connection between variables. After that we can do some prelimenary assumptions to our results.

```{r}
cor_human<-cor(human2) %>% round(digits=2)
cor_human 
```

```{r}
#Visualize correlation matrix with a plot
title <- "Correlations between variables in human data"

corrplot(cor_human, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex=0.6, title = title,  mar=c(0,0,1,0))
```

*Figure 1. Correlations between variables in human data*

In figure 1we see that HDI has the highest correlations of all, but that is expected since it is a combination variable of many others. This shows, that HDI works as it's is supposed to; it is able to combine variables concerning a long and healthy life and a long education to it. In addition it also reveals that not taking care of mother's influences to general human development negatively. 

More precisely, mothers' high mortality rate influences negatively to life expectancy in general. It is not as strong as HDI, but quite strong nevertheless (-0.74). When more mothers die, the expected years of education lower, too. When taking a sub group of adolescent mothers, the interpretation is similar.

With these correlations I can assume that HDI, mothers' mortality rate and adolescent birth rate determine tge expected years of education. In other words: if a state can make its citizens have a long and healthy life, it increases level's of education, too.

#3 Method: classification
Our data concerns broad view on human capacity and in my opinion, education is one of the best ways to increase human capacity. If we want to use fully our potential human capacity, we need to find out what are the structures that determine the level of education. It is in the hands of politicians to determine what the best level of education is but it is the work of science to find out what are the reasons why some people have a certain level of education.

Because I'm interested in expected levels of education, I believe classification method is the best analysing tool for that. I can divide the levels of education to groups and find out what variables determine which group. I don't aim to solve fully what are the reasons for such and such education level, but with our data we can use Linear Discriminant Analysis (LDA) to find out what are the variables that determine the groups of education in our data. 

##3.1 Wrangling Human

But first we need to wrangle the data in order to make analysing with LDA possible. Let's find out what we need to do by looking the summary of the data.

*Summary of the data*
```{r}
summary(human2)
```
We have all sorts of values in our variables from under a 1 or from 1 to 1100. In classification method, we're interested in the distances of the points in the data. In order to make these distances comparable, we need to scale the data.

*Distribution*
```{r}
#Draw a density plot of each variable
gather(human2) %>% ggplot(aes(value,fill="#99999",alpha=0.3)) + facet_wrap("key", scales = "free") + 
  geom_density() +
  ggtitle("Density plots of the variables in the human data") +
  theme(plot.title = element_text(hjust = 0.5,size=16,face='bold'),legend.position="none")
```

*Figure 2. Density plots of the variables in the human data*

LDA assumes that variables are normally distributed. In figure 2 we see that mothers' mortality rate and adolescent birth rates are biased both to the left. All other are reasonably normally distributed.

### scaling human

*scaled human*
```{r}
# center and standardize variables
human_scaled <- scale(human2)

# summaries of the scaled variables
summary(human_scaled)

# class of the human_scaled object
class(human_scaled)

#change the object to data frame
human_scaled <- as.data.frame(human_scaled)
class(human_scaled)
```
Now we have values only between -5 to 5. It is also transformed back to data frame, so further analysis can be made.

###Creating levels of education
Now all we need is the target variable from the expected level of education. What we need is to divide our expected level of education to four classes based on expected years of education.

*Factor variable*

```{r}

# save the Edu.exp as scaled_eduexp
scaled_eduexp <- human_scaled$Edu.exp

# summary of the scaled_eduexp
summary(scaled_eduexp)

# create a quantile vector of eduexp and print it
bins <- quantile(scaled_eduexp)
bins

# create a new categorical variable 'eduexp'
eduexp <- cut(scaled_eduexp, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor
table(eduexp)

# remove original Edu.exp from the dataset
human_scaled <- dplyr::select(human_scaled, -Edu.exp)

# add the new categorical value to scaled data
human_scaled <- data.frame(human_scaled, eduexp)
str(human_scaled)
```
Now we have a new categorical variable "eduexp" instead of continuous "Edu.exp" in our scaled data set. It has levels low, med_low, med_high and high based on how the years are divided quantiles in our real data. The top 25% belongs to high education and bottom 25% to low class.



###Test set and train set

In order to divide our education level to groups in the original data we need a training test to create the 4 groups. We also need a test set to find out how well our model actually predicts the wanted groups in the data. Our model is created by train set and tested with test set.

We'll divide the whole data so, that randomly selected 80% of the data is used for the training and rest 20% for testing. We also need to exclude and save the original observations from expected level of education. Since that is what we want to compare to, it cannot be in the testing set.

*Test and train*
```{r}
# number of rows in the human dataset 
n <- nrow(human_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- human_scaled[ind,]

# create test set 
test <- human_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$eduexp
summary(correct_classes)

# remove the eduexp variable from test data
test <- dplyr::select(test, -eduexp)
```

##3.2 LDA
Now we're ready to use the Linear Discriminant Analysis (LDA) in order to separate the data to groups. We will use train set to find those variables that best separate our target variable and test set to check how well our model predicts the classes.

In our LDA we have categorical expected years of education as a target variable against all other variables. Here we use our train set to create the model. Now we're ready to run the LDA, and it will create a model to predict expected years of education using all other variables. (The function for arrows in the plot was provided in our data camp exercises.)

*Model 1*
```{r}
lda.fit <- lda(formula= eduexp ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$eduexp)

# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch=classes, main = "Figure 3. LDA model 1, classes")
lda.arrows(lda.fit, myscale = 1)
#another plot with arrows showing clearly
plot(lda.fit, dimen=2, cex = c(0.1), main = "Figure 4. LDA model 1, arrows")
lda.arrows(lda.fit, myscale = 2)


```

Our plot for the LDA is actually also a dimensionally reduced way of picturing the results. It reduces our 10 variable plots to 2 components: LD1 and LD2. LD1 captures 92% of between group variance and LD2 is left with 6%.

I have created two plots to picture the model: figure 3 emphasizes how well the groups are separated by giving each group a different colour and figure 4 emphasizes the variables as arrows. The longer the arrow and narrower the angle between axis and an arrow, the stronger the connection between LD components and variables. The more separated groups the better the model.

In figure 3 we can see that the high end and the low end are best separated. In figure 4 we notice that HDI has the biggest connection to LD1 and so captures the largest variance between our groups of expected years of education. In other words, HDI is our strongest variable that separates the groups. Mother's mortality rate is the second strongest explanatory variable and strongly connected to LD2.

But since HDI is a combination variable based on already existing variables, I want to test what can be seen without it. That is why I'll exclude it in the second model, so it won't cloud the interpretation of the results. So here we will do just as before but just without the HDI.

*Data without HDI*

```{r}
human_scaled2 <-human_scaled
human_scaled2 <- dplyr::select(human_scaled2, -HDI)
```

*Test and training set without HDI*

```{r}
# number of rows in the human dataset 
n2 <- nrow(human_scaled2)

# choose randomly 80% of the rows
ind2 <- sample(n2,  size = n * 0.8)

# create train set
train2 <- human_scaled2[ind2,]

# create test set 
test2 <- human_scaled2[-ind2,]

# save the correct classes from test data
correct_classes2 <- test2$eduexp
summary(correct_classes2)

# remove the eduexp variable from test data
test2 <- dplyr::select(test2, -eduexp)
```

*Model 2, whithout HDI*
```{r}
lda.fit2 <- lda(formula= eduexp ~ ., data = train2)

# print the lda.fit object
lda.fit2

# the function for lda biplot arrows
lda.arrows2 <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes2 <- as.numeric(train2$eduexp)

# plot the lda results
plot(lda.fit2, dimen = 2, col=classes2, pch=classes2, main = "Figure 5. LDA model 2, classes")
lda.arrows2(lda.fit2, myscale = 1)

#another plot with arrows showing
plot(lda.fit2, dimen=2, cex = c(0.1), main = "Figure 6. LDA model 2, arrows")
lda.arrows2(lda.fit2, myscale = 2)
```

As we can see in figure 5, without HDI our groups are a bit more mixed up, especially in med low and med high levels of education. In figure 6, we notice that mothers' mortality seems to be in great value, capturing second most variance from the data and determining how to group the education levels. When comparing to model one, we see that mean years of education is only one even closely related to LD1. That means that HDI in the first model was mainly based on years of education and life expectancy at birth. 

So if the state already has high level of education, it is expected to continue. There is no big surprise in that, so in the sense of our interpretation, mothers' wellbeing and general life expectansies are more interesting results, although statistically less valuable.

What is more visible in the second model is the negative correlation between mother's mortality and education ratio from women to men. The higher the mother's mortality rate, the less women get to over second level education. 

Since the mothers' mortality has a lot stronger connection to LD2 than edu2FM, I'd say that it is the wellbeing of mothers' that count in estimating both expected years of education and the ratio from women to men in education level instead the other way around. The gender inequality in education levels starts already when mothers' mortality rate is high and effects to education levels all together. Unfortunately the mother's mortality rate wasn't distributed perfectly normally. This might change the interpretation of the results a bit and we need to consider its value in caution.

*Model 2 in 3D*
```{r}
model_predictors <- dplyr::select(train2, -eduexp)
# check the dimensions
dim(model_predictors)
dim(lda.fit2$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit2$scaling
matrix_product <- as.data.frame(matrix_product)
library(plotly)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color=train2$eduexp)
```
Figure 7. Model 2 in 3D

In figure 7 we can see the groups in 3D in our LDA model. So instead of first 2 components it catches first 3 components from model 2. The 3 components catch 99% of the between group variance. If you turn around the model you can see clear groups with slight mix in the edges of the group centres.

##3.3 Predicting power of the models
We have interpreted the results but are the results valid? This is the point where we need our test set to test how well our models predict the data not used for training.

*Model 1*
```{r}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
table(correct = correct_classes, predicted = lda.pred$class) %>% prop.table()%>%addmargins()
```


*Model 2*
```{r}
# predict classes with test data
lda.pred2 <- predict(lda.fit2, newdata = test2)

# cross tabulate the results
table(correct = correct_classes2, predicted = lda.pred2$class)
table(correct = correct_classes2, predicted = lda.pred2$class) %>% prop.table()%>%addmargins()
```




The model1 with HDI predicts better, but in this version we have some variables in a way twice. That's why I believe that the second model is more interesting, as we can see more detailed results.

Also, even in the second case, our results are quite good in the low and high end. All classes have 19-25%, when the original split was even 25% (the exact numbers vary since it calculates it every time randomly all over again). It seems that same variables explain both med low and med high education levels and shows that our model isn't that good in predicting the middle years. 

#4 clustering and LDA

I was also interested if our classification with expected years of education was reasonable. That's why I decided to test clustering method as addition to our interpretation.

Clustering is an unsupervised method which divides our data to groups without knowing the classes beforehand. In this exercise we will use k-means, which is one of the most used methods to perform clustering.

But first, we need to scale our data again in order to calculate distances between observations properly. We will do that without HDI, since we discovered that model 2 is more detailed in describing data.

*Summary of the scaled human3*
```{r}

human_scaled3 <- scale(human2)
human_scaled3 <- as.data.frame(human_scaled3)
human_scaled3 <- dplyr::select(human_scaled3, -HDI)

summary(human_scaled3)
class(human_scaled3)
human_scaled3<- as.data.frame(human_scaled3)
```
##4.1 k-means

Because clustering (with k-means) is an unsupervised method, we do not know the optimal number of clusters beforehand. That's why we use WCSS (total within sum of squares) to find out the optimal number of clusters. That's when in figure 8 the line drops rapidly. In this case, it's 2 clusters. When two clusters, the observations are the closest to cluster centre.

```{r}
dist_eu <- dist(human_scaled3)

# look at the summary of the distances
summary(dist_eu)
```

```{r}
library(ggplot2); library(GGally)
set.seed(123)

# euclidean distance matrix

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(dist_eu, k)$tot.withinss})

# visualize the results
plot(1:k_max, twcss, type='b', main="Results of the total whithin sum of squares")
```
*Figure 8. Results of the total whithin sum of squares*

Now let's do kmeans clustering with 2 centers.

```{r}
# k-means clustering
km <-kmeans(dist_eu, centers = 2)

human_scaled3$km <- as.factor(km$cluster)

# plot the human dataset with clusters
ggpairs(human_scaled3,  ggplot2::aes(colour=km), title="Paired variables with clusters")

```
*Figure 9. Paired variables with clusters*


If we compare these clusters from figure 9 to our previous classification results, we find out that expected years of education is strongly correlated to life expectancy (0.78) as in classification, but highest to mean years of education (0.83). It is also correlated to both mother's mortality (0.73) and adolescent births (0.70) as in classification method. Although, adolescent birth rate has a stronger influence than it has in classification. 


##4.2 LDA with k-means
Although our optimal amount of clusters was 2, I will test it with the same amount of clusters as was in our classification. Then I will perform LDA with 4 clusters and clusters as target variable. Then I will compare the results of the LDA with classification method.

```{r}
# k-means again
set.seed(123)
km2 <- kmeans(dist_eu, centers = 4)

# LDA with using the k-means clusters as target classes
human_scaled3$cl <- km2$cluster
lda.fit3 <- lda(cl ~ ., data = human_scaled3)
lda.fit3
plot(lda.fit3, col=as.numeric(human_scaled3$cl), dimen=2, main = "Figure 9. LDA model 3")
lda.arrows(lda.fit3, myscale = 2, col = "#666666")
```

In our LDA model 3 we have a lot of similarities to models 1 and 2 with the classification method. Figure 9 shows that mother's mortality rate is again almost exactly lined with LD2 and collects most of the variation within the groups in the data (after mean years of education). Also mother's mortality is negatively correlated to ratio of women to men in higher education: when women die in greater numbers they have more struggle to get to higher education.

As we saw in our classification method, mothers' mortality, life expectancy and mean years of education are the most significant variables. When performing classification with clusters instead of education levels, we found out those same factors determine the groups. Clustering did confirm valid our original separation based on levels of education and the key variables.


#5 Conclusive remarks

I conclude that mothers' mortality rate, life expectancy and mean years of education are the key variables when determining the level of expected years in education as was expected. Adolescent birth rate did not stand out as an important factor as was expected in hypothesis level. 

Both mothers' mortality rate and adolescent birth rate were bit curved to the left, so these results might not be as strong as suggested. With bigger data sample this hypotheses could be tested further.

Surprisingly enough, mothers' mortality is more important factor than Gross national income. It seems that it is more important to take care of the mothers than money if we want highest possible education. So in order to flourish, nations need to take care of their women.

Happy womens' day!
