---
title: "IODS Final Assignment"
author: "Maika"
date: "28 helmikuuta 2017"
email: "maija.absetz@helsinki.fi"
output: 
  html_document:
    theme: readable
    highlight: tango
    toc: true
    toc_depth: 2
    toc_floath: true
    fig_caption: true
    fig_width: 6
    fig_height: 5
    code_folding: hide
  
---


#1 Introduction to open data science
##1.1 What have we learned?

This is a progress of a Bachelor level historian developing her skills with statistics with r-software. I have statistics as a minor and just started with R programming language. My message to all of you is: If I can do this, so can you. I'm not saying it's easy, but I'm saying it's possible.

This course has been an introductory course to R-software using R-studio, Github and DataCamp. Through these weeks we have learned how to use R.markdown files and how to push changes with Github from R.mrakdown files to an html web page. We have also learned different data wrangling and analysing techniques briefly with data camp: [link](https://www.datacamp.com/)

What I've done so far can be seen in my course diary: [link](https://macabset.github.io/IODS-project/). We have had weekly assignments regarding a different data analysing method each week.

##1.2 What lies ahead of us
This is the final assignment for the course introduction to open data science. In this assignment we create our own web pages from the scratch. (Oh, yes, you're reading it.)

At my Github repository everyone can see the R.markdown codes for this web page. Basically it means that if you have Rstudio and Github username, you can copy the text and code from my Github file, so YOU can make your own web page, too. How cool is that! Link to my Github repository is as promised: [link](https://github.com/macabset/IODS-final)

If you keep up with me (and I recommend that you will), you will learn how real life data is wrangled so that classes and groups from the data can be calculated and visualized. I'll do my best to explain every step adequately.

#2 Data: human

I'm going to use my human data from previous exercises. The dataset originates from United Nations Development Programme, and you can read more about it from here: [link](http://hdr.undp.org/en/content/human-development-index-hdi). 

I find this data the most interesting one we've used, since it's actual data that describes the wellbeing of nations in a very multilinear way. Instead of just using Gross National Product or Index as an explaining variable, we can take inequality or education levels as explanatory variables when explaining the wellbeing of a state. This way we do not compare nations simply based on their wealth but based on their human capacity. 

##2.1 Variables

I have created a subset called "human2", since we have already used human data previously. The data consists of 10 variables and 155 observations. Variables modified from original dataset are Country, Edu2FM, LabFM and GNI. All variables are numeric and continuous. For further information about modifications made, you can go and check my Github repository: [link](https://github.com/macabset/IODS-project/blob/master/data/create_human.R)

The data wrangling done here is strongly based on the original ch4 workout. In addition I have added variable HDI and mean years of education. I hope these additions will provide more of background knowledge to the evaluations of the connections. Later on I will continue data wrangling for the analysing part.

Underneath are the names of the 10 variables and their description.

  + Country: used as row names, regions not included

  + Edu2FM: ratio of females to males in over secondary level education
  + LabFM: ratio of female labour force to male labour force
  + Edu.exp: Expected years of education
  + Life.exp: Life expectancy at birth
  + GNI: Gross National Income per capita
  + Mot.mor: Maternal mortality ratio
  + Adol.birth: Adolescent birth rate
  + Adol.birth: Percentage of female representatives in parliament
  + HDI: human development index. Combination of education, life expectation and GNI.
  + Edu.y.mean: mean years of education/ Country

In addition I have excluded missing values from our data set by removing them.

```{r}
#Packages we need in this workout
library(GGally)
library(dplyr)
library(corrplot)
library(FactoMineR)
library(ggplot2)
library(tidyr)
library(MASS)
```


```{r}
getwd()
human2 <- read.csv("data/human2.csv", sep=",", header = TRUE, row.names = 1)

str(human2)
dim(human2)
complete.cases(human2)
head(human2)
```

##2.2 Picturing the data

Let's do some correlation matrices and plots to visualize them in order to notice the connection between variables.

```{r}
cor_human<-cor(human2) %>% round(digits=2)
cor_human 

```

PUT TITLE IN THE PLOT: "Correlation between variables in human data"
```{r}
#Visualize correlation matrix with a plot
corrplot(cor_human, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex=0.6)
title("Correlation between variables", line= -30)

```
Figure 1. Correlations between variables in human data

HDI has the highest correlations of all, but that is expected since it is a combination variable of many others. This shows, that HDI works as it's is supposed to; it is able to combine variables concerning a long and healthy life and a long education to it. In addition it also reveals that not taking care of mother's influences to general human development negatively. 

More precisely, mothers' high mortality rate influences negatively to life expectancy in general. It is not as strong as HDI, but quite strong nevertheless (-0.74). When more mothers die, the expected years of education lower, too. When taking a sub group of adolescent mothers, the interpretation is similar.


#3 Method: classification
Our data concerns broad view on human capacity and in my opinion, education is one of the best ways to increase human capacity. If we want to use fully our potential human capacity, we need to find out what are the structures that determine the level of education. It is in the hands of politicians to determine what the best level of education is but it is the work of science to find out what are the reasons why some people have a certain level of education.

Because I'm interested in expected levels of education, I believe classification method is the best analysing tool for that. I can divide the levels of education to groups and find out what variables determine which group. I don't aim to solve fully what are the reasons for such and such education level, but with our data we can use Linear Discriminant Analysis (LDA) to find out what are the variables that determine the groups of education in our data. 

##3.1 Wrangling Human

But first we need to wrangle the data in order to make analysing with LDA possible. Let's find out what we need to do by looking the summary of the data.

*summary of the data*
```{r}
summary(human2)
```
We have all sorts of values in our variables from under a 1 or from 1 to 1100. In classification method, we're interested in the distances of the points in the data. In order to make these distances comparable, we need to scale the data.

*Distribution*
```{r}
#Draw a density plot of each variable
gather(human2) %>% ggplot(aes(value,fill="#99999",alpha=0.3)) + facet_wrap("key", scales = "free") + 
  geom_density() +
  ggtitle("Density plots of the variables in the human data") +
  theme(plot.title = element_text(hjust = 0.5,size=16,face='bold'),legend.position="none")
```
Figure 2. Density plots of the variables in the human data

LDA assumes that variables are normally distributed. In this case, mothers' mortality rate and adolescent birth rates are biased both to the left. All other are reasonably normally distributed.

### scaling human

*scaled human*
```{r}
# center and standardize variables
human_scaled <- scale(human2)

# summaries of the scaled variables
summary(human_scaled)

# class of the human_scaled object
class(human_scaled)

#change the object to data frame
human_scaled <- as.data.frame(human_scaled)
class(human_scaled)
```
Now we have values only between -5 to 5. It is also transformed back to data frame, so further analysis can be made.

###Creating levels of education
Now all we need is the target variable from the expected level of education. What we need is to divide our expected level of education to four classes based on expected years of education.

*factor variable*

```{r}

# save the Edu.exp as scaled_eduexp
scaled_eduexp <- human_scaled$Edu.exp

# summary of the scaled_eduexp
summary(scaled_eduexp)

# create a quantile vector of eduexp and print it
bins <- quantile(scaled_eduexp)
bins

# create a new categorical variable 'eduexp'
eduexp <- cut(scaled_eduexp, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor
table(eduexp)

# remove original Edu.exp from the dataset
human_scaled <- dplyr::select(human_scaled, -Edu.exp)

# add the new categorical value to scaled data
human_scaled <- data.frame(human_scaled, eduexp)
str(human_scaled)
```
Now we have a new categorical variable "eduexp" instead of continuous "Edu.exp" in our scaled data set. It has levels low, med_low, med_high and high based on how the years are divided quantiles in our real data. The top 25% belongs to high education and bottom 25% to low class.



###Test set and train set

In order to divide our education level to groups in the original data we need a training test to create the 4 groups. We also need a test set to find out how well our model actually predicts the wanted groups in the data. Our model is created by train set and tested with test set.

We'll divide the whole data so, that randomly selected 80% of the data is used for the training and rest 20% for testing. We also need to exclude and save the original observations from expected level of education. Since that is what we want to compare to, it cannot be in the testing set.

*test and train*
```{r}
# number of rows in the human dataset 
n <- nrow(human_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- human_scaled[ind,]

# create test set 
test <- human_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$eduexp
summary(correct_classes)

# remove the eduexp variable from test data
test <- dplyr::select(test, -eduexp)
```

##3.2 LDA
Now we're ready to use the Linear Discriminant Analysis (LDA) in order to separate the data to groups. We will use train set to find those variables that best separate our target variable and test set to check how well our model predicts the classes.

In our LDA we have categorical expected years of education as a target variable against all other variables. Here we use our train set to create the model. Now we're ready to run the LDA, and it will create a model to predict expected years of education using all other variables. (The function for arrows in the plot was provided in our data camp exercises.)

*Model 1*
```{r}
lda.fit <- lda(formula= eduexp ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$eduexp)

# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch=classes, main = "Figure 3. LDA model 1, classes")
lda.arrows(lda.fit, myscale = 1)
#another plot with arrows showing clearly
plot(lda.fit, dimen=2, cex = c(0.1), main = "Figure 4. LDA model 1, arrows")
lda.arrows(lda.fit, myscale = 2)


```
Our plot for the LDA is actually also a dimensionally reduced way of picturing the results. It reduces our 10 variable plots to 2 components: LD1 and LD2. LD1 captures 92% of between group variance and LD2 is left with 6%.

I have created two plots to picture the model: first one to emphasize how well the groups are separated by giving each group a different colour and second plot to emphasize the variables as arrows. The longer the arrow and narrower the angle between axis and an arrow, the stronger the connection between LD components and variables. The more separated groups the better the model.

Expected years of education have been predicted as levels by the LDA model and shown as a plot above. It shows the groups low to high in different colours and arrows as original variables that determine the groups. In our plot "1" we can see that the high end and the low end are best separated. In our plot "2" we can notice that HDI has the biggest connection to LD1 and so captures the largest variance between our groups of expected years of education. In other words, HDI is our strongest variable that separates the groups. Mother's mortality rate is the second strongest explanatory variable and strongly connected to LD2.

But since HDI is a combination variable based on already existing variables, I want to test what can be seen without it. That is why I'll exclude it in the second model, so it won't cloud the interpretation of the results. So here we will do just as before but just without the HDI.

*Datawithout HDI*

```{r}
human_scaled2 <-human_scaled
human_scaled2 <- dplyr::select(human_scaled2, -HDI)
```
*Test and training set without HDI*

```{r}
# number of rows in the human dataset 
n2 <- nrow(human_scaled2)

# choose randomly 80% of the rows
ind2 <- sample(n2,  size = n * 0.8)

# create train set
train2 <- human_scaled2[ind2,]

# create test set 
test2 <- human_scaled2[-ind2,]

# save the correct classes from test data
correct_classes2 <- test2$eduexp
summary(correct_classes2)

# remove the eduexp variable from test data
test2 <- dplyr::select(test2, -eduexp)
```

*Model 2, whithout HDI*
```{r}
lda.fit2 <- lda(formula= eduexp ~ ., data = train2)

# print the lda.fit object
lda.fit2

# the function for lda biplot arrows
lda.arrows2 <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes2 <- as.numeric(train2$eduexp)

# plot the lda results
plot(lda.fit2, dimen = 2, col=classes2, pch=classes2, main = "Figure 5. LDA model 2, classes")
lda.arrows2(lda.fit2, myscale = 1)

#another plot with arrows showing
plot(lda.fit2, dimen=2, cex = c(0.1), main = "Figure 6. LDA model 2, arrows")
lda.arrows2(lda.fit2, myscale = 2)
```

As we can see in picture "1", without HDI our groups are a bit more mixed up, especially in med low and med high levels of education. In picture "2", we notice that mothers' mortality seems to be in great value, capturing second most variance from the data and determining how to group the education levels. When comparing to model one, we see that mean years of education is only one even closely related to LD1. That means that HDI in the first model was mainly based on years of education and life expectancy at birth.

What is more visible in the second model is the negative correlation between mother's mortality and education ratio from women to men. The higher the mother's mortality rate, the less women get to over second level education. 

Since the mothers' mortality has a lot stronger connection to LD2 than edu2FM, I'd say that it is the wellbeing of mothers' that count in estimating both expected years of education and the ratio from women to men in education level instead the other way around. The gender inequality in education levels starts already when mothers' mortality rate is high and effects to education levels all together.

*Model 2 in 3D*
```{r}
model_predictors <- dplyr::select(train2, -eduexp)
# check the dimensions
dim(model_predictors)
dim(lda.fit2$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit2$scaling
matrix_product <- as.data.frame(matrix_product)
library(plotly)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color=train2$eduexp)
```
Figure 7. Model 2 in 3D

In our plot "x" we can see the groups in 3D in our LDA model. So instead of first 2 components it catches first 3 components from model 2. If you turn around the model you can see clear groups with slight mix in the edges of the group centres.

##3.3 Predicting power of the models
We have interpreted the results but are the results valid? This is the point where we need our test set to test how well our models predict the data not used for training.

*Model 1*
```{r}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
table(correct = correct_classes, predicted = lda.pred$class) %>% prop.table()%>%addmargins()
```


*Model 2*
```{r}
# predict classes with test data
lda.pred2 <- predict(lda.fit2, newdata = test2)

# cross tabulate the results
table(correct = correct_classes2, predicted = lda.pred2$class)
table(correct = correct_classes2, predicted = lda.pred2$class) %>% prop.table()%>%addmargins()
```




The model1 with HDI predicts better, but in this version we have some variables in a way twice. That's why I believe that the second model is more interesting, as we can see more detailed results.

Also, even in the second case, our results are quite good in the low and high end. All classes have 19-25%, when the original split was even 25% (the exact numbers vary since it calculates it every time randomly all over again). It seems that same variables explain both med low and med high education levels and shows that our model isn't that good in predicting the middle years. 

#4 clustering and LDA

I was also interested if our classification with expected years of education was reasonable. That's why I decided to test clustering method as addition to our interpretation.

Clustering is an unsupervised method which divides our data to groups without knowing the classes beforehand. In this exercise we will use k-means, which is one of the most used methods to perform clustering.

But first, we need to scale our data again in order to calculate distances between observations properly. We will do that without HDI, since we discovered that it is more detailed in describing data.

*Summary of the scaled human3*
```{r}

human_scaled3 <- scale(human2)
human_scaled3 <- as.data.frame(human_scaled3)
human_scaled3 <- dplyr::select(human_scaled3, -HDI)

summary(human_scaled3)
class(human_scaled3)
human_scaled3<- as.data.frame(human_scaled3)
```
##4.1 k-means

Because clustering (with k-means) is an unsupervised method, we do not know the optimal number of clusters beforehand. That's why we use WCSS (total within sum of squares) to find out the optimal number of clusters. That's when in plot "x" the line drops rapidly. In this case, it's 2 clusters. When two clusters, the observations are the closest to cluster centre.

```{r}
dist_eu <- dist(human_scaled3)

# look at the summary of the distances
summary(dist_eu)
```

```{r}
library(ggplot2); library(GGally)
set.seed(123)

# euclidean distance matrix

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(dist_eu, k)$tot.withinss})

# visualize the results
plot(1:k_max, twcss, type='b', main="Results of the total whithin sum of squares")
```
Figure 8. Results of the total whithin sum of squares

Now let's do kmeans clustering with 2 centers.

```{r}
# k-means clustering
km <-kmeans(dist_eu, centers = 2)

human_scaled3$km <- as.factor(km$cluster)

# plot the human dataset with clusters
ggpairs(human_scaled3,  ggplot2::aes(colour=km))

```
If we compare these clusters to our previous classification results, we find out that expected years of education is strongly correlated to life expectancy (0.78) as in classification, but highest to mean years of education (0.83). It is also correlated to both mother's mortality (0.73) and adolescent births (0.70) as in classification method. Although, adolescent birth rate has a stronger influence than it has in classification. 

Although our optimal amount of clusters was 2, I will test it with the same amount of clusters as was in our classification. Then I will perform LDA with 4 clusters and clusters as target variable. Then I will compare the results of the LDA with classification method.

##4.2 LDA with k-means

```{r}
# k-means again
set.seed(123)
km2 <- kmeans(dist_eu, centers = 4)

# LDA with using the k-means clusters as target classes
human_scaled3$cl <- km2$cluster
lda.fit3 <- lda(cl ~ ., data = human_scaled3)
lda.fit3
plot(lda.fit3, col=as.numeric(human_scaled3$cl), dimen=2, main = "Figure x. LDA model 3")
lda.arrows(lda.fit3, myscale = 2, col = "#666666")
```

In our LDA model 3 we have a lot of similarities to models 1 and 2 with the classification method. Mother's mortality is again almost exactly lined with LD2 and collects most of the variation within the groups in the data. Also mother's mortality is negatively correlated to ratio of women to men in higher education.

As we saw in our classification method, mothers' mortality, life expectancy and mean years of education are the most significant variables. Although when performing classification with clusters instead of education levels, we found out those same factors determine the groups. Clustering did confirm valid our original separation based on levels of education and the key variables.


#5 Conclusive remarks


I conclude that mothers' mortality rate, life expectancy and mean years of education are the key variables when determining the level of expected years in education

Surprisingly enough mothers' mortality is more important factor than Gross national index. It seems that it is more important to take care of the mothers than money if we want highest possible education. So in order to flourish, nations need to take care of their women.

